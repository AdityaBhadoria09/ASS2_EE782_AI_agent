{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70748722",
   "metadata": {},
   "source": [
    "### 1.Prayash Kumar Sahu(22B1261)\n",
    "### 2.Aditya Singh Bhadoria(22B1247)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a20719",
   "metadata": {},
   "source": [
    "### Github Link: https://github.com/AdityaBhadoria09/ASS2_EE782_AI_agent.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6385deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install deepface opencv-python speechrecognition pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3472eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyaudioNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading PyAudio-0.2.14-cp38-cp38-win_amd64.whl.metadata (2.7 kB)\n",
      "Downloading PyAudio-0.2.14-cp38-cp38-win_amd64.whl (164 kB)\n",
      "   ---------------------------------------- 0.0/164.1 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/164.1 kB ? eta -:--:--\n",
      "   ---- ---------------------------------- 20.5/164.1 kB 217.9 kB/s eta 0:00:01\n",
      "   ---- ---------------------------------- 20.5/164.1 kB 217.9 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 41.0/164.1 kB 217.9 kB/s eta 0:00:01\n",
      "   -------------- ------------------------ 61.4/164.1 kB 251.0 kB/s eta 0:00:01\n",
      "   -------------------------------------  163.8/164.1 kB 653.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- 164.1/164.1 kB 578.3 kB/s eta 0:00:00\n",
      "Installing collected packages: pyaudio\n",
      "Successfully installed pyaudio-0.2.14\n"
     ]
    }
   ],
   "source": [
    "pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9acb222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acb42595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e957eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\pashu\\AppData\\Roaming\\Python\\Python313\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Processing trusted faces...\n",
      "✅ Aditya.jpg processed with augmentation.\n",
      "--- Generated 1 trusted embeddings ---\n",
      "\n",
      "Processing random faces...\n",
      "✅ ee782_ass2_pic2.jpg processed with augmentation.\n",
      "✅ ee782_ass2_pic3.jpg processed with augmentation.\n",
      "✅ ee782_ass2_pic6.jpg processed with augmentation.\n",
      "✅ ee782_ass2_pic7.jpg processed with augmentation.\n",
      "✅ WIN_20251021_23_35_22_Pro.jpg processed with augmentation.\n",
      "--- Generated 5 random embeddings ---\n",
      "\n",
      "✅ All embeddings saved to embeddings.npz\n"
     ]
    }
   ],
   "source": [
    "import os  # Used for interacting with the operating system (e.g., listing files in a directory)\n",
    "import cv2  # OpenCV: For image reading, manipulation, and augmentations\n",
    "import numpy as np  # NumPy: For numerical operations, especially with arrays (embeddings)\n",
    "from deepface import DeepFace  # The core library for face recognition and embedding generation\n",
    "\n",
    "# ------------------------------\n",
    "# Configuration Constants\n",
    "# ------------------------------\n",
    "# Define the face recognition model to use.\n",
    "# 'Facenet' is a popular and robust model developed by Google.\n",
    "MODEL = \"Facenet\"\n",
    "\n",
    "# Define the face detector backend.\n",
    "# 'retinaface' is a high-accuracy detector, good at finding faces even in challenging conditions.\n",
    "DETECTOR = \"retinaface\"\n",
    "\n",
    "# ------------------------------\n",
    "# Image Augmentation Function\n",
    "# ------------------------------\n",
    "\n",
    "def augment_image(img):\n",
    "    \"\"\"\n",
    "    Generates a list of augmented (modified) versions of a single input image.\n",
    "    \n",
    "    These augmentations create variations in lighting, orientation, and scale,\n",
    "    which makes the final averaged embedding more robust to real-world conditions.\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): The original image read by cv2.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of np.ndarray images, starting with the original image\n",
    "              followed by all its augmented versions.\n",
    "    \"\"\"\n",
    "    # Get the original image dimensions\n",
    "    h, w = img.shape[:2]\n",
    "    \n",
    "    # Start the list of augmented images, beginning with the original\n",
    "    augmented = [img]\n",
    "\n",
    "    # --- 1. Flip ---\n",
    "    # Flip the image horizontally. This helps the model learn\n",
    "    # that a person is the same regardless of left/right orientation.\n",
    "    augmented.append(cv2.flip(img, 1))\n",
    "\n",
    "    # --- 2. Brightness ---\n",
    "    # Create darker (alpha=0.8) and brighter (alpha=1.2) versions.\n",
    "    # This simulates different lighting conditions.\n",
    "    for alpha in [0.8, 1.2]:\n",
    "        # cv2.convertScaleAbs changes the brightness (alpha) and contrast (beta)\n",
    "        bright = cv2.convertScaleAbs(img, alpha=alpha, beta=0)\n",
    "        augmented.append(bright)\n",
    "\n",
    "    # --- 3. Rotation ---\n",
    "    # Rotate the image slightly to simulate head tilt.\n",
    "    for angle in [-10, 10]:  # 10 degrees left and right\n",
    "        # Get the rotation matrix for rotating around the center of the image\n",
    "        M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1.0)\n",
    "        # Apply the rotation\n",
    "        rotated = cv2.warpAffine(img, M, (w, h))\n",
    "        augmented.append(rotated)\n",
    "\n",
    "    # --- 4. Scaling (Zoom) ---\n",
    "    # Simulate the person being closer or farther away.\n",
    "    for scale in [0.9, 1.1]:  # Zoom out (90%) and zoom in (110%)\n",
    "        # Resize the image\n",
    "        resized = cv2.resize(img, None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        # After scaling, we need to crop/pad it back to the original size (w, h)\n",
    "        # to maintain consistent dimensions.\n",
    "        \n",
    "        # Calculate top-left (x1, y1) coordinates for a center crop\n",
    "        y1 = max(0, (resized.shape[0] - h) // 2)\n",
    "        x1 = max(0, (resized.shape[1] - w) // 2)\n",
    "        \n",
    "        # Perform the crop\n",
    "        cropped = resized[y1:y1 + h, x1:x1 + w]\n",
    "        \n",
    "        # In case the 0.9 scale-down + crop resulted in a slightly smaller image\n",
    "        # (due to rounding), resize it exactly back to the original dimensions.\n",
    "        cropped = cv2.resize(cropped, (w, h))\n",
    "        augmented.append(cropped)\n",
    "\n",
    "    return augmented\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Single Embedding Computation\n",
    "# ------------------------------\n",
    "\n",
    "def get_embedding_from_image(img):\n",
    "    \"\"\"\n",
    "    Computes a single, L2-normalized embedding for a given image array.\n",
    "\n",
    "    This is a helper function called by `get_embedding`.\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): An image object (not a file path).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 1D L2-normalized embedding vector for the face in the image.\n",
    "    \n",
    "    Raises:\n",
    "        Exception: If DeepFace fails to generate an embedding.\n",
    "    \"\"\"\n",
    "    # DeepFace.represent finds the face and computes its vector representation (embedding).\n",
    "    # `enforce_detection=False`: If DeepFace can't find a face, it will try to\n",
    "    # process the whole image. This can be useful but may also lead to errors\n",
    "    # if the augmented image is too distorted, which is why we have a try/except\n",
    "    # block in the calling function `get_embedding`.\n",
    "    rep = DeepFace.represent(\n",
    "        img_path=img,\n",
    "        model_name=MODEL,\n",
    "        detector_backend=DETECTOR,\n",
    "        enforce_detection=False\n",
    "    )\n",
    "    \n",
    "    # The result `rep` is a list of dictionaries. We take the embedding from the first result.\n",
    "    emb = np.array(rep[0][\"embedding\"])\n",
    "    \n",
    "    # L2 Normalization (emb / np.linalg.norm(emb)):\n",
    "    # This scales the embedding vector to have a length of 1.\n",
    "    # This is a crucial step! Normalized vectors allow us to use\n",
    "    # cosine similarity (or dot product) to measure how \"close\" two faces are.\n",
    "    return emb / np.linalg.norm(emb)\n",
    "\n",
    "\n",
    "def get_embedding(img_path):\n",
    "    \"\"\"\n",
    "    Computes a robust, averaged embedding for a single image file.\n",
    "    \n",
    "    It does this by:\n",
    "    1. Reading the image from the file path.\n",
    "    2. Generating multiple augmentations of the image.\n",
    "    3. Computing an embedding for *each* augmentation.\n",
    "    4. Averaging all successful embeddings into a single \"mean\" embedding.\n",
    "    5. Normalizing the final mean embedding.\n",
    "    \n",
    "    This \"averaged\" embedding is much more reliable than one from a single pose/lighting.\n",
    "\n",
    "    Args:\n",
    "        img_path (str): The file path to the image.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The final L2-normalized, averaged embedding vector.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the image cannot be read or no valid embeddings\n",
    "                    could be generated from any of its augmentations.\n",
    "    \"\"\"\n",
    "    # Read the image from the specified path\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Cannot read {img_path}\")\n",
    "\n",
    "    # Generate all augmented versions\n",
    "    augmented_images = augment_image(img)\n",
    "    embeddings = []\n",
    "\n",
    "    # Loop through all augmented images (including the original)\n",
    "    for aug in augmented_images:\n",
    "        try:\n",
    "            # Try to get an embedding for this specific augmentation\n",
    "            emb = get_embedding_from_image(aug)\n",
    "            embeddings.append(emb)\n",
    "        except Exception as e:\n",
    "            # If DeepFace fails (e.g., rotation makes face undetectable),\n",
    "            # just print a warning and skip this augmentation.\n",
    "            print(f\"⚠️ Augmentation skipped for {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # After trying all augmentations, check if we got any successful results\n",
    "    if len(embeddings) == 0:\n",
    "        raise ValueError(f\"No valid embeddings generated for {img_path}.\")\n",
    "    \n",
    "    # Calculate the mean (average) embedding across all successful augmentations.\n",
    "    # `axis=0` computes the mean down each column, resulting in a single 1D vector.\n",
    "    mean_embedding = np.mean(embeddings, axis=0)\n",
    "    \n",
    "    # Normalize the final mean embedding as well, just to ensure it's a unit vector.\n",
    "    return mean_embedding / np.linalg.norm(mean_embedding)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Batch Processing Function\n",
    "# ------------------------------\n",
    "\n",
    "def compute_all(folder):\n",
    "    \"\"\"\n",
    "    Processes all images in a given folder and computes their robust embeddings.\n",
    "\n",
    "    Args:\n",
    "        folder (str): The path to the directory containing images.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - np.ndarray: A 2D array where each row is an embedding.\n",
    "            - list: A list of filenames corresponding to each embedding row.\n",
    "    \"\"\"\n",
    "    embeddings, names = [], []\n",
    "    valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".webp\")\n",
    "    \n",
    "    # Loop through every file in the specified folder\n",
    "    for f in os.listdir(folder):\n",
    "        # Check if the file has a valid image extension\n",
    "        if f.lower().endswith(valid_extensions):\n",
    "            path = os.path.join(folder, f)\n",
    "            try:\n",
    "                # Get the robust, averaged embedding for this image\n",
    "                emb = get_embedding(path)\n",
    "                embeddings.append(emb)\n",
    "                names.append(f)\n",
    "                print(f\"✅ {f} processed with augmentation.\")\n",
    "            except Exception as e:\n",
    "                # If `get_embedding` fails (e.g., image is corrupt or no face\n",
    "                # could be processed at all), skip this file.\n",
    "                print(f\"⚠️ {f} skipped: {e}\")\n",
    "                \n",
    "    # Convert the list of embeddings into a 2D NumPy array\n",
    "    return np.array(embeddings), names\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Main Execution Block\n",
    "# ------------------------------\n",
    "\n",
    "# This block only runs when the script is executed directly\n",
    "# (not when it's imported as a module).\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- Step 1: Process Trusted Faces ---\n",
    "    # These are the images of people you \"know\" or \"trust\".\n",
    "    print(\"Processing trusted faces...\")\n",
    "    trusted_embeddings, trusted_names = compute_all(\"trusted_faces\")\n",
    "    print(f\"--- Generated {len(trusted_embeddings)} trusted embeddings ---\")\n",
    "\n",
    "    # --- Step 2: Process Random Faces ---\n",
    "    # These are images of unknown people, used for threshold calibration or as negative samples.\n",
    "    print(\"\\nProcessing random faces...\")\n",
    "    random_embeddings, random_names = compute_all(\"random_faces\")\n",
    "    print(f\"--- Generated {len(random_embeddings)} random embeddings ---\")\n",
    "\n",
    "    # --- Step 3: Save Embeddings ---\n",
    "    # Save the computed embeddings to a compressed NumPy file (.npz).\n",
    "    # This allows you to load them quickly later without re-computing everything.\n",
    "    np.savez(\n",
    "        \"embeddings.npz\",\n",
    "        trusted=trusted_embeddings,\n",
    "        random=random_embeddings\n",
    "        # Note: We don't save the 'names' lists here, but you could:\n",
    "        # trusted_names=trusted_names,\n",
    "        # random_names=random_names\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✅ All embeddings saved to embeddings.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0d213c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Auto-calibrated threshold: 0.619\n",
      "🎥 Camera running. Press 'q' to exit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "from deepface import DeepFace\n",
    "\n",
    "# === Load embeddings ===\n",
    "data = np.load(\"embeddings.npz\")\n",
    "trusted_embeddings = data[\"trusted\"]\n",
    "random_embeddings = data[\"random\"]\n",
    "\n",
    "# Compute centroid of trusted faces\n",
    "trusted_centroid = np.mean(trusted_embeddings, axis=0)\n",
    "trusted_centroid /= np.linalg.norm(trusted_centroid)\n",
    "\n",
    "# Determine dynamic threshold based on separation\n",
    "# Compute average distance between trusted centroid and random faces\n",
    "def cosine_sim(a, b): return np.dot(a, b) / (np.linalg.norm(a)*np.linalg.norm(b))\n",
    "random_sims = [cosine_sim(trusted_centroid, r) for r in random_embeddings]\n",
    "trusted_sims = [cosine_sim(trusted_centroid, t) for t in trusted_embeddings]\n",
    "\n",
    "# Example: choose threshold halfway between mean trusted and random similarities\n",
    "THRESHOLD = (np.mean(trusted_sims) + np.mean(random_sims)) / 2\n",
    "print(f\"🔹 Auto-calibrated threshold: {THRESHOLD:.3f}\")\n",
    "\n",
    "# === Real-time detection ===\n",
    "cap = cv2.VideoCapture(0)\n",
    "last_unknown_save = 0\n",
    "SAVE_COOLDOWN = 10\n",
    "unknown_dir = \"unknown_faces\"\n",
    "os.makedirs(unknown_dir, exist_ok=True)\n",
    "\n",
    "print(\"🎥 Camera running. Press 'q' to exit.\")\n",
    "\n",
    "frame_count = 0  # <--- initialize before the loop\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count % 3 != 0:   # <-- skip every 2 out of 3 frames\n",
    "        continue\n",
    "\n",
    "    # ↓ Resize frame before running face detection (faster RetinaFace)\n",
    "    frame = cv2.resize(frame, (480, 360))    \n",
    "\n",
    "    try:\n",
    "        detections = DeepFace.extract_faces(\n",
    "            img_path=frame,\n",
    "            detector_backend=\"opencv\",\n",
    "            enforce_detection=False\n",
    "        )\n",
    "    except Exception:\n",
    "        detections = []\n",
    "\n",
    "    for det in detections:\n",
    "        face = det[\"face\"]\n",
    "        area = det[\"facial_area\"]\n",
    "        x, y, w, h = area[\"x\"], area[\"y\"], area[\"w\"], area[\"h\"]\n",
    "\n",
    "        # Compute embedding\n",
    "        try:\n",
    "            rep = DeepFace.represent(\n",
    "                img_path=face,\n",
    "                model_name=\"Facenet\",\n",
    "                detector_backend=\"skip\",  # already cropped\n",
    "                enforce_detection=False\n",
    "            )\n",
    "            emb = np.array(rep[0][\"embedding\"])\n",
    "            emb = emb / np.linalg.norm(emb)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        # Compare with trusted centroid\n",
    "        sim = cosine_sim(emb, trusted_centroid)\n",
    "        if sim > THRESHOLD:\n",
    "            label = f\"TRUSTED ({sim:.2f})\"\n",
    "            color = (0, 255, 0)\n",
    "        else:\n",
    "            label = f\"UNKNOWN ({sim:.2f})\"\n",
    "            color = (0, 0, 255)\n",
    "            now = time.time()\n",
    "            if now - last_unknown_save > SAVE_COOLDOWN:\n",
    "                ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                fname = f\"unknown_{ts}.jpg\"\n",
    "                cv2.imwrite(f\"{unknown_dir}/{fname}\", face)\n",
    "                print(f\"💾 Unknown saved: {fname}\")\n",
    "                print(\"You are not authorized!! Please Leave!\")\n",
    "                last_unknown_save = now\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "        cv2.putText(frame, label, (x, y - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "\n",
    "    cv2.imshow(\"Face Verification\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
