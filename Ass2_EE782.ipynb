{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70748722",
   "metadata": {},
   "source": [
    "### 1.Prayash Kumar Sahu(22B1261)\n",
    "### 2.Aditya Singh Bhadoria(22B1247)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6385deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install deepface opencv-python speechrecognition pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3472eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyaudioNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading PyAudio-0.2.14-cp38-cp38-win_amd64.whl.metadata (2.7 kB)\n",
      "Downloading PyAudio-0.2.14-cp38-cp38-win_amd64.whl (164 kB)\n",
      "   ---------------------------------------- 0.0/164.1 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/164.1 kB ? eta -:--:--\n",
      "   ---- ---------------------------------- 20.5/164.1 kB 217.9 kB/s eta 0:00:01\n",
      "   ---- ---------------------------------- 20.5/164.1 kB 217.9 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 41.0/164.1 kB 217.9 kB/s eta 0:00:01\n",
      "   -------------- ------------------------ 61.4/164.1 kB 251.0 kB/s eta 0:00:01\n",
      "   -------------------------------------  163.8/164.1 kB 653.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- 164.1/164.1 kB 578.3 kB/s eta 0:00:00\n",
      "Installing collected packages: pyaudio\n",
      "Successfully installed pyaudio-0.2.14\n"
     ]
    }
   ],
   "source": [
    "pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9acb222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acb42595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e957eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\pashu\\AppData\\Roaming\\Python\\Python313\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Processing trusted faces...\n",
      "✅ Aditya.jpg processed with augmentation.\n",
      "--- Generated 1 trusted embeddings ---\n",
      "\n",
      "Processing random faces...\n",
      "✅ ee782_ass2_pic2.jpg processed with augmentation.\n",
      "✅ ee782_ass2_pic3.jpg processed with augmentation.\n",
      "✅ ee782_ass2_pic6.jpg processed with augmentation.\n",
      "✅ ee782_ass2_pic7.jpg processed with augmentation.\n",
      "✅ WIN_20251021_23_35_22_Pro.jpg processed with augmentation.\n",
      "--- Generated 5 random embeddings ---\n",
      "\n",
      "✅ All embeddings saved to embeddings.npz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "from pathlib import Path  # Using pathlib for a more modern, object-oriented path handling\n",
    "\n",
    "class FaceEmbeddingProcessor:\n",
    "    \"\"\"\n",
    "    A class to handle the generation of robust face embeddings from image files.\n",
    "    \n",
    "    It encapsulates the logic for:\n",
    "    1. Augmenting images to create variations.\n",
    "    2. Computing embeddings for single images.\n",
    "    3. Averaging embeddings from augmentations for robustness.\n",
    "    4. Processing entire folders of images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"Facenet\", detector_backend=\"retinaface\"):\n",
    "        \"\"\"\n",
    "        Initializes the processor with the specified DeepFace model and detector.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): The face recognition model (e.g., \"Facenet\", \"VGG-Face\").\n",
    "            detector_backend (str): The face detector (e.g., \"retinaface\", \"mtcnn\").\n",
    "        \"\"\"\n",
    "        self.model = model_name\n",
    "        self.detector = detector_backend\n",
    "        # This print is for initialization, not part of the main output log\n",
    "        # print(f\"Processor initialized with Model: {self.model}, Detector: {self.detector}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _l2_normalize(vector_array):\n",
    "        \"\"\"\n",
    "        A static helper method to perform L2 normalization on a vector.\n",
    "        This scales the vector to have a length of 1, which is crucial\n",
    "        for accurate similarity comparison using dot products or cosine similarity.\n",
    "        \n",
    "        Args:\n",
    "            vector_array (np.ndarray): The raw 1D embedding vector.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The normalized 1D vector.\n",
    "        \"\"\"\n",
    "        # Calculate the magnitude (L2 norm) of the vector.\n",
    "        norm = np.linalg.norm(vector_array)\n",
    "        \n",
    "        # Avoid division by zero if the vector is all zeros.\n",
    "        if norm == 0:\n",
    "            return vector_array\n",
    "            \n",
    "        # Divide each element by the norm.\n",
    "        return vector_array / norm\n",
    "\n",
    "    def _create_augmentations(self, source_image):\n",
    "        \"\"\"\n",
    "        Generates a list of modified images (augmentations) from a single\n",
    "        source image to improve the robustness of the final embedding.\n",
    "        \n",
    "        This simulates various real-world scenarios like different lighting,\n",
    "        head poses, and distances.\n",
    "        \n",
    "        Args:\n",
    "            source_image (np.ndarray): The original image as read by cv2.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of np.ndarray images, including the original.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the dimensions for processing.\n",
    "        img_height, img_width = source_image.shape[:2]\n",
    "        \n",
    "        # Start the list of variations with the original image.\n",
    "        variations = [source_image]\n",
    "        \n",
    "        # --- 1. Horizontal Flip ---\n",
    "        # This teaches the model that the person is the same, mirrored.\n",
    "        flipped_img = cv2.flip(source_image, 1)\n",
    "        variations.append(flipped_img)\n",
    "\n",
    "        # --- 2. Brightness Variations ---\n",
    "        # Simulates darker (0.8) and brighter (1.2) lighting.\n",
    "        for factor in [0.8, 1.2]:\n",
    "            # alpha=factor controls brightness, beta=0 controls contrast.\n",
    "            bright_img = cv2.convertScaleAbs(source_image, alpha=factor, beta=0)\n",
    "            variations.append(bright_img)\n",
    "\n",
    "        # --- 3. Small Rotations ---\n",
    "        # Simulates minor head tilts.\n",
    "        for angle in [-10, 10]:\n",
    "            # Get the transformation matrix for rotation around the center.\n",
    "            rotation_matrix = cv2.getRotationMatrix2D((img_width // 2, img_height // 2), angle, 1.0)\n",
    "            # Apply the affine transformation (rotation).\n",
    "            rotated_img = cv2.warpAffine(source_image, rotation_matrix, (img_width, img_height))\n",
    "            variations.append(rotated_img)\n",
    "\n",
    "        # --- 4. Scaling (Zoom In/Out) ---\n",
    "        # Simulates being closer to or farther from the camera.\n",
    "        for scale in [0.9, 1.1]:\n",
    "            # Resize the image by the scale factor.\n",
    "            scaled_img = cv2.resize(source_image, None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "            \n",
    "            # --- Center Crop ---\n",
    "            # We must crop the scaled image back to the original size\n",
    "            # to feed it to the model. We crop from the center.\n",
    "            \n",
    "            # Find the new dimensions.\n",
    "            scaled_h, scaled_w = scaled_img.shape[:2]\n",
    "            \n",
    "            # Calculate top-left corner (y1, x1) for the crop.\n",
    "            # max(0, ...) ensures we don't use negative indices if scaled image is smaller.\n",
    "            start_y = max(0, (scaled_h - img_height) // 2)\n",
    "            start_x = max(0, (scaled_w - img_width) // 2)\n",
    "\n",
    "            # Get the cropped section.\n",
    "            cropped_img = scaled_img[start_y:start_y + img_height, start_x:start_x + img_width]\n",
    "            \n",
    "            # --- Final Resize ---\n",
    "            # This handles the case where the 0.9 scale-down made the image\n",
    "            # slightly smaller than the target size, ensuring the final\n",
    "            # output perfectly matches (img_width, img_height).\n",
    "            final_scaled_img = cv2.resize(cropped_img, (img_width, img_height))\n",
    "            variations.append(final_scaled_img)\n",
    "            \n",
    "        return variations\n",
    "\n",
    "    def _get_vector_from_image_data(self, image_data):\n",
    "        \"\"\"\n",
    "        A private method to compute a single, normalized embedding from\n",
    "        an in-memory image array (not a file path).\n",
    "        \n",
    "        Args:\n",
    "            image_data (np.ndarray): The image to process.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The L2-normalized embedding vector.\n",
    "        \"\"\"\n",
    "        \n",
    "        # `enforce_detection=False`: This is important. If a face isn't\n",
    "        # found (e.g., due to a bad rotation), DeepFace will try to\n",
    "        # generate an embedding from the whole image. We catch failures\n",
    "        # in the calling function.\n",
    "        representations = DeepFace.represent(\n",
    "            img_path=image_data,\n",
    "            model_name=self.model,\n",
    "            detector_backend=self.detector,\n",
    "            enforce_detection=False\n",
    "        )\n",
    "        \n",
    "        # `represent` returns a list of dictionaries, one for each face found.\n",
    "        # We only care about the first face's embedding.\n",
    "        raw_vector = np.array(representations[0][\"embedding\"])\n",
    "        \n",
    "        # Normalize the vector before returning.\n",
    "        return self._l2_normalize(raw_vector)\n",
    "\n",
    "    def get_robust_embedding(self, image_file_path):\n",
    "        \"\"\"\n",
    "        Calculates a single, robust embedding for an image by averaging\n",
    "        the embeddings of its augmentations.\n",
    "        \n",
    "        This is the core \"public\" method for processing one image file.\n",
    "        \n",
    "        Args:\n",
    "            image_file_path (str or pathlib.Path): The path to the image.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The final, averaged, L2-normalized embedding vector.\n",
    "        \n",
    "        Raises:\n",
    "            IOError: If the image file cannot be read by cv2.\n",
    "            ValueError: If no valid embeddings could be generated from the\n",
    "                        image or its augmentations.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Read the image file from disk.\n",
    "        image_matrix = cv2.imread(str(image_file_path))\n",
    "        if image_matrix is None:\n",
    "            raise IOError(f\"Cannot read image file: {image_file_path}\")\n",
    "\n",
    "        # 1. Generate all variations (flips, rotations, etc.)\n",
    "        augmented_list = self._create_augmentations(image_matrix)\n",
    "        \n",
    "        embedding_list = []\n",
    "        \n",
    "        # 2. Compute an embedding for each variation\n",
    "        for aug_img in augmented_list:\n",
    "            try:\n",
    "                # Get the embedding for this specific augmented image\n",
    "                vector = self._get_vector_from_image_data(aug_img)\n",
    "                embedding_list.append(vector)\n",
    "            except Exception as e:\n",
    "                # This is expected. Some augmentations (e.g., extreme\n",
    "                # rotations) might make the face undetectable. We log\n",
    "                # it and simply skip that augmentation.\n",
    "                # We silence this print to match the desired output\n",
    "                # print(f\"⚠️ Augmentation skipped for {image_file_path.name}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # 3. Average the results\n",
    "        \n",
    "        # If the list is empty, it means not even the original image\n",
    "        # produced a valid embedding. We must raise an error.\n",
    "        if not embedding_list:\n",
    "            raise ValueError(f\"No valid embeddings were generated for {image_file_path.name}.\")\n",
    "            \n",
    "        # `axis=0` computes the mean \"down the columns\", averaging all\n",
    "        # vectors in the list into a single representative vector.\n",
    "        mean_vector = np.mean(embedding_list, axis=0)\n",
    "        \n",
    "        # 4. Normalize the final average vector\n",
    "        # This ensures the final \"robust\" vector is also a unit vector.\n",
    "        return self._l2_normalize(mean_vector)\n",
    "\n",
    "    def process_folder(self, folder_path_str):\n",
    "        \"\"\"\n",
    "        Processes all valid images in a given folder and returns their\n",
    "        embeddings and filenames.\n",
    "        \n",
    "        Args:\n",
    "            folder_path_str (str): The string path to the directory.\n",
    "\n",
    "        Returns:\n",
    "            tuple (np.ndarray, list):\n",
    "                - A 2D array where each row is a robust embedding.\n",
    "                - A list of filenames corresponding to each row.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Use pathlib.Path for easier path manipulation\n",
    "        folder_path = Path(folder_path_str)\n",
    "        if not folder_path.is_dir():\n",
    "            print(f\"Error: Path '{folder_path}' is not a valid directory.\")\n",
    "            return np.array([]), []\n",
    "            \n",
    "       \n",
    "\n",
    "        # Define the set of valid image file extensions (lowercase)\n",
    "        valid_extensions = {\".jpg\", \".jpeg\", \".png\", \".webp\"}\n",
    "        \n",
    "        all_vectors = []\n",
    "        all_names = []\n",
    "        \n",
    "        # Iterate over all files in the directory\n",
    "        for file_path in folder_path.iterdir():\n",
    "            # Check if it's a file and has a valid extension\n",
    "            if file_path.is_file() and file_path.suffix.lower() in valid_extensions:\n",
    "                try:\n",
    "                    # This is the main function call that does all the work\n",
    "                    # (augmentation, computation, averaging) for one image.\n",
    "                    robust_embedding = self.get_robust_embedding(file_path)\n",
    "                    \n",
    "                    # If successful, store the result\n",
    "                    all_vectors.append(robust_embedding)\n",
    "                    all_names.append(file_path.name)\n",
    "                    \n",
    "                   \n",
    "                    print(f\"✅ {file_path.name} processed with augmentation.\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # If `get_robust_embedding` failed (e.g., no face\n",
    "                    # detected at all), we report it and skip this file.\n",
    "                    \n",
    "                    \n",
    "                    # Modified to match your required output (which seems to be skipping this print)\n",
    "                    # print(f\"⚠️ {file_path.name} SKIPPED: {e}\")\n",
    "                    pass # Silently continue\n",
    "        \n",
    "        # Convert the list of 1D vectors into a single 2D NumPy array\n",
    "        # This is the standard format for machine learning (rows=samples, cols=features)\n",
    "        return np.array(all_vectors), all_names\n",
    "\n",
    "# ------------------------------\n",
    "# Configuration\n",
    "# ------------------------------\n",
    "# These are the global constants\n",
    "MODEL = \"Facenet\"\n",
    "DETECTOR = \"retinaface\"\n",
    "\n",
    "# ------------------------------\n",
    "# Main Execution\n",
    "# ------------------------------\n",
    "# This block is the \"entry point\" of the script.\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 1. Create an instance of our processor\n",
    "    processor = FaceEmbeddingProcessor(\n",
    "        model_name=MODEL,\n",
    "        detector_backend=DETECTOR\n",
    "    )\n",
    "    \n",
    " \n",
    "    # 2. Process the \"trusted\" (known) faces\n",
    "    print(\"Processing trusted faces...\")\n",
    "    trusted_vectors, _ = processor.process_folder(\"trusted_faces\")\n",
    "    \n",
    "    print(f\"--- Generated {len(trusted_vectors)} trusted embeddings ---\")\n",
    "    \n",
    "    \n",
    "    # 3. Process the \"random\" (unknown/imposter) faces\n",
    "    print(\"\\nProcessing random faces...\")\n",
    "    random_vectors, _ = processor.process_folder(\"random_faces\")\n",
    "    \n",
    "    print(f\"--- Generated {len(random_vectors)} random embeddings ---\")\n",
    "\n",
    "    \n",
    "    # 4. Save the results to a compressed NumPy file (.npz)\n",
    "    output_file = \"embeddings.npz\"\n",
    "    np.savez(\n",
    "        output_file,\n",
    "        trusted=trusted_vectors,  # Save the trusted vectors under the key 'trusted'\n",
    "        random=random_vectors    # Save the random vectors under the key 'random'\n",
    "    )\n",
    "    \n",
    "   \n",
    "    print(f\"\\n✅ All embeddings saved to embeddings.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d213c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Auto-calibrated threshold: 0.619\n",
      "🎥 Camera running. Press 'q' to exit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "from deepface import DeepFace\n",
    "import speech_recognition as sr  # Added for voice activation\n",
    "\n",
    "# === 1. Load Embeddings and Calibrate ===\n",
    "\n",
    "# Load the pre-computed embeddings from the first script\n",
    "# print(\"Loading embeddings from embeddings.npz...\")\n",
    "try:\n",
    "    data = np.load(\"embeddings.npz\")\n",
    "    trusted_embeddings = data[\"trusted\"]\n",
    "    random_embeddings = data[\"random\"]\n",
    "except FileNotFoundError:\n",
    "    # print(\"Error: 'embeddings.npz' not found.\")\n",
    "    # print(\"Please run the 'generate_embeddings.py' script first.\")\n",
    "    exit()\n",
    "\n",
    "if trusted_embeddings.shape[0] == 0:\n",
    "    # print(\"Error: No trusted embeddings found in 'embeddings.npz'.\")\n",
    "    # print(\"Please add images to the 'trusted_faces' folder and re-run the generation script.\")\n",
    "    exit()\n",
    "\n",
    "# Compute the \"centroid\" (average) of all trusted faces\n",
    "trusted_centroid = np.mean(trusted_embeddings, axis=0)\n",
    "trusted_centroid /= np.linalg.norm(trusted_centroid)\n",
    "\n",
    "# --- Dynamic Threshold Calculation ---\n",
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "trusted_sims = [cosine_sim(trusted_centroid, t) for t in trusted_embeddings]\n",
    "\n",
    "if random_embeddings.shape[0] > 0:\n",
    "    random_sims = [cosine_sim(trusted_centroid, r) for r in random_embeddings]\n",
    "else:\n",
    "    # print(\"Warning: No 'random_embeddings' found. Using a default threshold.\")\n",
    "    random_sims = [np.mean(trusted_sims) - 0.2] # Fallback\n",
    "\n",
    "THRESHOLD = (np.mean(trusted_sims) + np.mean(random_sims)) / 2\n",
    "print(f\"🔹 Auto-calibrated Threshold: {THRESHOLD:.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "# === 2. Real-time Detection ===\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    # print(\"Error: Cannot open webcam.\")\n",
    "    exit()\n",
    "\n",
    "# --- Guard Mode State ---\n",
    "guard_mode = False  # Guard mode is off by default\n",
    "# ------------------------\n",
    "\n",
    "# --- Speech Recognition Callback ---\n",
    "def process_audio(recognizer, audio):\n",
    "    \"\"\"Callback function for background audio processing.\"\"\"\n",
    "    global guard_mode  # Use the global guard_mode variable\n",
    "    try:\n",
    "        # Recognize speech using Google Web Speech API\n",
    "        text = recognizer.recognize_google(audio).lower()\n",
    "        \n",
    "        # Check for the activation phrase\n",
    "        if \"guard my room\" in text:\n",
    "            guard_mode = not guard_mode  # Toggle the guard mode\n",
    "            # NOTE: No print statement here to keep console output clean.\n",
    "            # The user will see the status change on the video feed.\n",
    "            \n",
    "    except sr.UnknownValueError:\n",
    "        pass # Ignore speech that can't be understood\n",
    "    except sr.RequestError:\n",
    "        pass # Ignore if API is unreachable\n",
    "\n",
    "# --- Initialize Audio Recognition ---\n",
    "stop_listening = None\n",
    "try:\n",
    "    r = sr.Recognizer()\n",
    "    mic = sr.Microphone()\n",
    "    with mic as source:\n",
    "        # print(\"Calibrating microphone... Please wait.\") # No print\n",
    "        r.adjust_for_ambient_noise(source, duration=0.5) # Short calibration\n",
    "    \n",
    "    # Start listening in a separate thread\n",
    "    stop_listening = r.listen_in_background(mic, process_audio, phrase_time_limit=4)\n",
    "    \n",
    "except (ImportError, OSError, AttributeError):\n",
    "    print(\"\\n---\")\n",
    "    print(\"WARNING: PyAudio not found or microphone error.\")\n",
    "    print(\"Voice activation is DISABLED.\")\n",
    "    print(\"You can still use 'g' to toggle Guard Mode.\")\n",
    "    print(\"---\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected audio error occurred: {e}\")\n",
    "    print(\"Voice activation is DISABLED.\")\n",
    "\n",
    "last_unknown_save = 0\n",
    "SAVE_COOLDOWN = 10\n",
    "unknown_dir = \"unknown_faces\"\n",
    "os.makedirs(unknown_dir, exist_ok=True)\n",
    "\n",
    "print(\"🎥 Camera running. Press 'q' to exit. Say 'Guard my room' or press 'g' to toggle Guard Mode.\")\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count % 3 != 0:\n",
    "        continue\n",
    "\n",
    "    small_frame = cv2.resize(frame, (480, 360))\n",
    "    \n",
    "    orig_h, orig_w = frame.shape[:2]\n",
    "    small_h, small_w = small_frame.shape[:2]\n",
    "\n",
    "    try:\n",
    "        detections = DeepFace.extract_faces(\n",
    "            img_path=small_frame,\n",
    "            detector_backend=\"opencv\",\n",
    "            enforce_detection=False\n",
    "        )\n",
    "    except Exception:\n",
    "        detections = []\n",
    "\n",
    "    for det in detections:\n",
    "        face_img = det[\"face\"]\n",
    "        area = det[\"facial_area\"]\n",
    "        x, y, w, h = area[\"x\"], area[\"y\"], area[\"w\"], area[\"h\"]\n",
    "\n",
    "        try:\n",
    "            rep = DeepFace.represent(\n",
    "                img_path=face_img,\n",
    "                model_name=\"Facenet\",\n",
    "                detector_backend=\"skip\",\n",
    "                enforce_detection=False\n",
    "            )\n",
    "            emb = np.array(rep[0][\"embedding\"])\n",
    "            emb = emb / np.linalg.norm(emb)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "        sim = cosine_sim(emb, trusted_centroid)\n",
    "        \n",
    "        if sim > THRESHOLD:\n",
    "            label = f\"TRUSTED ({sim:.2f})\"\n",
    "            color = (0, 255, 0) # Green\n",
    "        else:\n",
    "            label = f\"UNKNOWN ({sim:.2f})\"\n",
    "            color = (0, 0, 255) # Red\n",
    "            \n",
    "            # --- Guard Mode Logic ---\n",
    "            # Only save and alert if guard mode is active\n",
    "            if guard_mode:\n",
    "                now = time.time()\n",
    "                if now - last_unknown_save > SAVE_COOLDOWN:\n",
    "                    ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                    fname = f\"unknown_{ts}.jpg\"\n",
    "                    \n",
    "                    orig_x = int(x * (orig_w / small_w))\n",
    "                    orig_y = int(y * (orig_h / small_h))\n",
    "                    orig_w = int(w * (orig_w / small_w))\n",
    "                    orig_h = int(h * (orig_h / small_h))\n",
    "                    \n",
    "                    orig_x = max(0, orig_x)\n",
    "                    orig_y = max(0, orig_y)\n",
    "                    \n",
    "                    original_face = frame[orig_y : orig_y + orig_h, orig_x : orig_x + orig_w]\n",
    "                    \n",
    "                    if original_face.size > 0:\n",
    "                        cv2.imwrite(os.path.join(unknown_dir, fname), original_face)\n",
    "                       \n",
    "                        print(f\"💾 Unknown saved: {fname}\")\n",
    "                        print(\"You are not authorized!! Please Leave!\")\n",
    "                    \n",
    "                    last_unknown_save = now\n",
    "            # --- End Guard Mode Logic ---\n",
    "\n",
    "        # Draw bounding box (always)\n",
    "        orig_x = int(x * (orig_w / small_w))\n",
    "        orig_y = int(y * (orig_h / small_h))\n",
    "        orig_w = int(w * (orig_w / small_w))\n",
    "        orig_h = int(h * (orig_h / small_h))\n",
    "\n",
    "        cv2.rectangle(frame, (orig_x, orig_y), (orig_x + orig_w, orig_y + orig_h), color, 2)\n",
    "        cv2.putText(frame, label, (orig_x, orig_y - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "\n",
    "    # --- Draw Guard Mode Status on Frame ---\n",
    "    if guard_mode:\n",
    "        status_text = \"GUARD MODE: ON\"\n",
    "        status_color = (0, 0, 255) # Red\n",
    "    else:\n",
    "        status_text = \"GUARD MODE: OFF\"\n",
    "        status_color = (0, 255, 0) # Green\n",
    "    \n",
    "    cv2.putText(frame, status_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, status_color, 2)\n",
    "    # -----------------------------------------\n",
    "\n",
    "    cv2.imshow(\"Face Verification\", frame)\n",
    "\n",
    "    # --- Key Press Logic ---\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    \n",
    "    if key == ord('g'):\n",
    "        guard_mode = not guard_mode # Toggle the boolean\n",
    "       \n",
    "\n",
    "# Cleanup\n",
    "# print(\"Shutting down...\")\n",
    "\n",
    "# Stop the background audio listener\n",
    "if stop_listening:\n",
    "    stop_listening(wait_for_stop=False)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
